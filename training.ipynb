{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training",
   "id": "406a275748fc4ae1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-12T12:29:53.484634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import cross_val_score, HalvingGridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from utils.text_preprocess import LemmaTokenizer\n",
    "\n",
    "df = pd.read_csv('data/amazon_reviews.txt', sep='\\t')\n",
    "df['VERIFIED_PURCHASE'] = df['VERIFIED_PURCHASE'].map({'Y': 1, 'N': 0})\n",
    "df['REVIEW_LENGTH'] = df['REVIEW_TEXT'].apply(len)\n",
    "\n",
    "classifiers = [\n",
    "    LinearSVC(random_state=42),\n",
    "    MultinomialNB(),\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "]\n",
    "vectorizers = [\n",
    "    TfidfVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None),\n",
    "    CountVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None, max_features=500),\n",
    "]\n",
    "\n",
    "word2vec_model_file =  'models/word2vec_200.model'\n",
    "model = Word2Vec.load(word2vec_model_file)\n",
    "\n",
    "def average_word_embeddings(sentence, model):\n",
    "    words = LemmaTokenizer()(sentence)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Define a function to process each review\n",
    "def process_review(review_text, model):\n",
    "    sentences = sent_tokenize(review_text)\n",
    "    sentence_embeddings = [average_word_embeddings(sentence, model) for sentence in sentences]\n",
    "    return sentence_embeddings\n",
    "\n",
    "df['Sentence_Embeddings'] = df['REVIEW_TEXT'].apply(lambda x: process_review(x, model))\n",
    "\n",
    "class DenseTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray() if hasattr(X, 'toarray') else X\n",
    "\n",
    "class SentenceEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X should be a list of numpy arrays (each array representing sentence embeddings)\n",
    "        transformed = []\n",
    "        for embeddings in X:\n",
    "            # If embeddings is a list, convert it to a numpy array\n",
    "            if isinstance(embeddings, list):\n",
    "                embeddings = np.array(embeddings)\n",
    "            \n",
    "            if embeddings.size > 0:\n",
    "                # Return the average embedding of the sentences\n",
    "                transformed.append(np.mean(embeddings, axis=0))\n",
    "            else:\n",
    "                # Handle case where there are no embeddings\n",
    "                transformed.append(np.zeros(embeddings.shape[1]))\n",
    "\n",
    "        return np.array(transformed)\n",
    "pipes = []\n",
    "pipes_names = []\n",
    "for clf in classifiers:\n",
    "    for vectorizer in vectorizers:\n",
    "        for useEmbeddings in [True, False]:\n",
    "            for useVP in [True, False]:\n",
    "                transformers = [\n",
    "                    ('vectorizer', vectorizer, 'REVIEW_TEXT'),\n",
    "                    ('length', 'passthrough', ['REVIEW_LENGTH'])\n",
    "                ]\n",
    "                if useEmbeddings:\n",
    "                    transformers.append(('embeddings', SentenceEmbeddingTransformer(), 'Sentence_Embeddings'))\n",
    "                if useVP:\n",
    "                    transformers.append(('encoder', OneHotEncoder(), ['VERIFIED_PURCHASE']))\n",
    "                \n",
    "                if isinstance(clf, MultinomialNB):\n",
    "                    # Add DenseTransformer after ColumnTransformer to convert the full sparse matrix to dense\n",
    "                    pipes.append(Pipeline([\n",
    "                        ('preprocessor', ColumnTransformer(transformers, n_jobs=-1)),\n",
    "                        ('to_dense', DenseTransformer()),  # Apply DenseTransformer here\n",
    "                        ('clf', clf)\n",
    "                    ]))\n",
    "                else:\n",
    "                    # No need for DenseTransformer for other classifiers\n",
    "                    pipes.append(Pipeline([\n",
    "                        ('preprocessor', ColumnTransformer(transformers, n_jobs=-1)),\n",
    "                        ('clf', clf)\n",
    "                    ]))\n",
    "                    \n",
    "                pipes_names.append(f'{clf.__class__.__name__}_'\n",
    "                                   f'{vectorizer.__class__.__name__}_'\n",
    "                                   f'{\"WE\" if useEmbeddings else \"\"}_'\n",
    "                                   f'{\"VP\" if useVP else \"\"}')\n",
    "\n",
    "param_grids = {\n",
    "    'LinearSVC': {\n",
    "        'clf__C': np.arange(2, 10, 2),\n",
    "        'clf__max_iter': [2000, 4000],\n",
    "        'clf__loss': ['hinge', 'squared_hinge'],\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'clf__n_estimators': [100, 200, 400, 800],\n",
    "        'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'clf__max_depth': [None, 20, 40, 60],\n",
    "        'clf__bootstrap': [True],\n",
    "    },\n",
    "    'MultinomialNB': {\n",
    "        'clf__alpha': [0.0001, 0.001, 0.1, 0.5, 1.0],\n",
    "    }\n",
    "}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for pipe, pipe_name in zip(pipes, pipes_names):\n",
    "    print(pipe_name)\n",
    "    clf_name = pipe.steps[-1][1].__class__.__name__\n",
    "    grid = param_grids[clf_name]\n",
    "    clf = HalvingGridSearchCV(pipe, grid, cv=inner_cv, factor=2, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    scores = []\n",
    "    # Loop instead of cross_val_score to have more control over printing\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(df, df['LABEL'])):\n",
    "        print(f\"\\nFold {fold}:\")\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        y_train, y_test = df['LABEL'].iloc[train_idx], df['LABEL'].iloc[test_idx]\n",
    "        tmp = (pipe.named_steps['preprocessor'].fit_transform(X_train))\n",
    "        print(tmp[0])\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(clf.score(X_test, y_test))\n",
    "        \n",
    "        print(f\"Best params: {clf.best_params_}\")\n",
    "        print(f\"Best score: {clf.best_score_}\")\n",
    "        print(f\"Test score: {scores[-1]}\")\n",
    "        \n",
    "    print('Accuracies: ' + str(scores) + '\\n'\n",
    "          + 'Mean Accuracy: ' + str(scores.mean()) + '\\n')\n"
   ],
   "id": "4e135cda31eea7a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\PycharmProjects\\amazon_fake_reviews_detection\\utils\\text_preprocess.py:30: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_TfidfVectorizer_WE_VP\n",
      "\n",
      "Fold 0:\n",
      "  (0, 852)\t0.3146327143841256\n",
      "  (0, 2572)\t0.33762293932767545\n",
      "  (0, 4106)\t0.2844732352491088\n",
      "  (0, 9077)\t0.29783850851486987\n",
      "  (0, 9493)\t0.39766840274834825\n",
      "  (0, 11251)\t0.2529684210489428\n",
      "  (0, 13232)\t0.230142245150199\n",
      "  (0, 14727)\t0.3888962647599638\n",
      "  (0, 15785)\t0.32145160084596025\n",
      "  (0, 17210)\t0.29616572577813016\n",
      "  (0, 19507)\t116.0\n",
      "  (0, 19508)\t1.0\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8f6c1a107f93329b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
