{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training",
   "id": "406a275748fc4ae1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prepare dataframe ",
   "id": "5185c4c214683a8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T13:48:30.857922Z",
     "start_time": "2024-09-13T13:48:03.061760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from nltk import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from utils.text_preprocess import LemmaTokenizer\n",
    "\n",
    "df = pd.read_csv('data/amazon_reviews.txt', sep='\\t')\n",
    "df['VERIFIED_PURCHASE'] = df['VERIFIED_PURCHASE'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "word2vec_model_file = 'models/word2vec_200.model'\n",
    "model = Word2Vec.load(word2vec_model_file)\n",
    "\n",
    "def average_word_embeddings(sentence, model):\n",
    "    words = LemmaTokenizer()(sentence)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Define a function to process each review\n",
    "def process_review(review_text, model):\n",
    "    sentences = sent_tokenize(review_text)\n",
    "    sentence_embeddings = [average_word_embeddings(sentence, model) for sentence in sentences]\n",
    "    return sentence_embeddings\n",
    "\n",
    "df['Sentence_Embeddings'] = df['REVIEW_TEXT'].apply(lambda x: process_review(x, model))\n"
   ],
   "id": "9d484cd9e4e6b1c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utente\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\Utente\\PycharmProjects\\amazon_fake_reviews_detection\\utils\\text_preprocess.py:30: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create pipelines",
   "id": "51b2984b39e567b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T13:48:30.880303Z",
     "start_time": "2024-09-13T13:48:30.866539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class DenseTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray() if hasattr(X, 'toarray') else X\n",
    "\n",
    "class SentenceEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X should be a list of numpy arrays (each array representing sentence embeddings)\n",
    "        transformed = []\n",
    "        for embeddings in X:\n",
    "            # If embeddings is a list, convert it to a numpy array\n",
    "            if isinstance(embeddings, list):\n",
    "                embeddings = np.array(embeddings)\n",
    "            \n",
    "            if embeddings.size > 0:\n",
    "                # Return the average embedding of the sentences\n",
    "                transformed.append(np.mean(embeddings, axis=0))\n",
    "            else:\n",
    "                # Handle case where there are no embeddings\n",
    "                transformed.append(np.zeros(embeddings.shape[1]))\n",
    "\n",
    "        return np.array(transformed)\n",
    "    \n",
    "classifiers = [\n",
    "    CalibratedClassifierCV(LinearSVC(random_state=42, dual='auto')),\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    MultinomialNB(),\n",
    "]\n",
    "vectorizers = [\n",
    "    TfidfVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None),\n",
    "    CountVectorizer(tokenizer=LemmaTokenizer(), token_pattern=None),\n",
    "]\n",
    "\n",
    "pipes = []\n",
    "pipes_names = []\n",
    "for clf in classifiers:\n",
    "    for vectorizer in vectorizers:\n",
    "        for useEmbeddings in [True, False]:\n",
    "            for useVP in [True, False]:\n",
    "                transformers = [\n",
    "                    ('vectorizer', vectorizer, 'REVIEW_TEXT')\n",
    "                ]\n",
    "                if useEmbeddings:\n",
    "                    transformers.append(('embeddings', SentenceEmbeddingTransformer(), 'Sentence_Embeddings'))\n",
    "                if useVP:\n",
    "                    transformers.append(('encoder', OneHotEncoder(), ['VERIFIED_PURCHASE']))\n",
    "                \n",
    "                if isinstance(clf, MultinomialNB):\n",
    "                    # Add DenseTransformer after ColumnTransformer to convert the full sparse matrix to dense\n",
    "                    pipes.append(Pipeline([\n",
    "                        ('preprocessor', ColumnTransformer(transformers, n_jobs=-1)),\n",
    "                        ('to_dense', DenseTransformer()),  # Apply DenseTransformer here\n",
    "                        ('clf', clf)\n",
    "                    ]))\n",
    "                else:\n",
    "                    # No need for DenseTransformer for other classifiers\n",
    "                    pipes.append(Pipeline([\n",
    "                        ('preprocessor', ColumnTransformer(transformers, n_jobs=-1)),\n",
    "                        ('clf', clf)\n",
    "                    ]))\n",
    "                    \n",
    "                pipes_names.append(f'{clf.__class__.__name__}_'\n",
    "                                   f'{vectorizer.__class__.__name__}'\n",
    "                                   f'_{\"WE\" if useEmbeddings else \"\"}'\n",
    "                                   f'_{\"VP\" if useVP else \"\"}')"
   ],
   "id": "b2c4919bcc0b9509",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nested cross-validation",
   "id": "9d80d859f9116171"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-13T13:48:31.029792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from nltk import sent_tokenize\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import cross_val_score, HalvingGridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from utils.text_preprocess import LemmaTokenizer\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    'CalibratedClassifierCV': {\n",
    "        'clf__estimator__C': [0.5, 1, 1.5],\n",
    "        'clf__estimator__max_iter': [2000, 4000],\n",
    "        'clf__estimator__loss': ['hinge', 'squared_hinge'],\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'clf__n_estimators': [100, 200, 400, 600],\n",
    "        'clf__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'clf__max_depth': [None, 20, 40, 60],\n",
    "        'clf__bootstrap': [False, True],\n",
    "    },\n",
    "    'MultinomialNB': {\n",
    "        'clf__alpha': [0.0001, 0.001, 0.1, 0.5, 1.0],\n",
    "    }\n",
    "}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for pipe, pipe_name in zip(pipes, pipes_names):\n",
    "    print(pipe_name)\n",
    "    clf_name = pipe.steps[-1][1].__class__.__name__\n",
    "    grid = param_grids[clf_name]\n",
    "    clf = HalvingGridSearchCV(pipe, grid, cv=inner_cv, factor=2, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Loop instead of cross_val_score to have more control over printing\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(df, df['LABEL'])):\n",
    "        print(f\"\\nFold {fold}:\")\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        y_train, y_test = df['LABEL'].iloc[train_idx], df['LABEL'].iloc[test_idx]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results[pipe_name] = [[], [], []]\n",
    "        results[pipe_name][0].append(accuracy_score(y_test, y_pred))\n",
    "        results[pipe_name][1].append(f1_score(y_test, y_pred, pos_label='__label1__'))\n",
    "        results[pipe_name][2].append(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]))\n",
    "        \n",
    "        # Update best model \n",
    "        if results[pipe_name][0][-1] > best_accuracy:\n",
    "            best_accuracy = results[pipe_name][0][-1]\n",
    "            best_model = clf.best_estimator_\n",
    "        joblib.dump(best_model, 'best_model.pkl')\n",
    "        print(f\"Best params: {clf.best_params_}\\n\"\n",
    "              f\"Test accuracy: { results[pipe_name][0][-1]}\\n\"\n",
    "              f\"Test F1: { results[pipe_name][1][-1]}\\n\"\n",
    "              f\"Test AUC: { results[pipe_name][2][-1]}\")\n",
    "    \n",
    "    print(f'Mean Accuracy: {np.mean(results[pipe_name][0])}\\n'\n",
    "          f'Mean F1: {np.mean(results[pipe_name][1])}\\n'\n",
    "          f'Mean AUC: {np.mean(results[pipe_name][2])}')\n",
    "\n",
    "# save results to csv file\n",
    "results_str = {key: [str(value) for value in values] for key, values in results.items()}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for key, value in results_str.items():\n",
    "    temp_df = pd.DataFrame([value], columns=['accuracy', 'f1_score', 'AUC'])\n",
    "    temp_df.insert(0, 'model', key)\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "df.to_csv('results.csv', index=False)\n",
    "\n",
    "# save best model\n",
    "joblib.dump(best_model, 'best_model.pkl')"
   ],
   "id": "4e135cda31eea7a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalibratedClassifierCV_TfidfVectorizer_WE_VP\n",
      "\n",
      "Fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__estimator__C': 1, 'clf__estimator__loss': 'hinge', 'clf__estimator__max_iter': 2000}\n",
      "Test accuracy: 0.8023809523809524\n",
      "Test F1: 0.7995169082125603\n",
      "Test AUC: 0.8620913832199546\n",
      "\n",
      "Fold 1:\n",
      "Best params: {'clf__estimator__C': 1, 'clf__estimator__loss': 'hinge', 'clf__estimator__max_iter': 4000}\n",
      "Test accuracy: 0.8023809523809524\n",
      "Test F1: 0.8006724303554275\n",
      "Test AUC: 0.8635798185941044\n",
      "\n",
      "Fold 2:\n",
      "Best params: {'clf__estimator__C': 1, 'clf__estimator__loss': 'hinge', 'clf__estimator__max_iter': 2000}\n",
      "Test accuracy: 0.8071428571428572\n",
      "Test F1: 0.8034934497816594\n",
      "Test AUC: 0.8738383219954649\n",
      "\n",
      "Fold 3:\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
